import * as tf from '@tensorflow/tfjs';
import { loadData } from './dataProcessor';

const opts= {
  originalDim: 28 * 28, 
  intermediateDim: 32, 
  latentDim: 2
}

class ZLayer extends tf.layers.Layer {
  constructor(config: any) {
    super(config);
  }

  computeOutputShape(inputShape: any) {
    tf.util.assert(inputShape.length === 2 && Array.isArray(inputShape[0]), () => `Expected exactly 2 input shapes. But got: ${inputShape}`);
    return inputShape[0];
  }

  call(inputs: tf.Tensor[], kwargs: any): tf.Tensor {
    const [zMean, zLogVar] = inputs;
    const batch: number = zMean.shape[0] || 1;
    const dim: number = zMean.shape[1] || 1;

    const mean = 0;
    const std = 1.0;
    // sample epsilon = N(0, I)
    const epsilon = tf.randomNormal([batch, dim], mean, std);

    // z = z_mean + sqrt(var) * epsilon
    return zMean.add(zLogVar.mul(0.5).exp().mul(epsilon));
  }

  static get className() {
    return 'ZLayer';
  }
}

function encoder(opts: { originalDim: number, intermediateDim: number, latentDim: number}): tf.LayersModel {
  const {originalDim, intermediateDim, latentDim} = opts;

  const inputs = tf.input({shape: [originalDim], name: 'encoder_input'});
  const x = tf.layers.dense({units: intermediateDim, activation: 'relu'}).apply(inputs);
  const zMean = tf.layers.dense({units: latentDim, name: 'z_mean'}).apply(x) as tf.SymbolicTensor;
  const zLogVar = tf.layers.dense({units: latentDim, name: 'z_log_var'}).apply(x) as tf.SymbolicTensor;

  const z = new ZLayer({name: 'z', outputShape: [latentDim]}).apply([zMean, zLogVar]) as tf.SymbolicTensor;

  const enc = tf.model({
    inputs: inputs,
    outputs: [zMean, zLogVar, z],
    name: 'encoder',
  });

  // console.log('Encoder Summary');
  enc.summary();
  return enc;
}

function decoder(opts: { originalDim: number, intermediateDim: number, latentDim: number}) {
  const {originalDim, intermediateDim, latentDim} = opts;

  const input = tf.input({shape: [latentDim]});

  let y = tf.layers.dense({
    units: intermediateDim,
    activation: 'relu'
  }).apply(input);

  y = tf.layers.dense({
    units: originalDim,
    activation: 'sigmoid'
  }).apply(y) as tf.SymbolicTensor;

  const dec = tf.model({inputs: input, outputs: y});

  // console.log('Decoder Summary');
  dec.summary();
  return dec;
}

function vae(encoder: any, decoder: any) {
  const inputs = encoder.inputs;
  const encoderOutputs = encoder.apply(inputs);
  const encoded = encoderOutputs[2];
  const decoderOutput = decoder.apply(encoded);
  const v = tf.model({
    inputs: inputs,
    outputs: [decoderOutput, ...encoderOutputs],
    name: 'vae_mlp',
  })

  // console.log('VAE Summary');
  v.summary();
  return v;
}

function vaeLoss(inputs: tf.Tensor, outputs: tf.Tensor[]): tf.Scalar {
  return tf.tidy(() => {
    const originalDim = inputs.shape[1] || 1;
    const decoderOutput = outputs[0];
    const zMean = outputs[1];
    const zLogVar = outputs[2];

    // First we compute a 'reconstruction loss' terms. The goal of minimizing
    // this term is to make the model outputs match the input data.
    const reconstructionLoss =
        tf.losses.meanSquaredError(inputs, decoderOutput).mul(originalDim);

    // binaryCrossEntropy can be used as an alternative loss function
    // const reconstructionLoss =
    //  tf.metrics.binaryCrossentropy(inputs, decoderOutput).mul(originalDim);

    // Next we compute the KL-divergence between zLogVar and zMean, minimizing
    // this term aims to make the distribution of latent variable more normally
    // distributed around the center of the latent space.
    let klLoss = zLogVar.add(1).sub(zMean.square()).sub(zLogVar.exp());
    klLoss = klLoss.sum(-1).mul(-0.5);

    return reconstructionLoss.add(klLoss).mean();
  });
}

async function train(vaeOpts: { originalDim: number, intermediateDim: number, latentDim: number}) {
  const images = await loadData();
  const encoderModel = encoder(vaeOpts);
  const decoderModel = decoder(vaeOpts);
  const vaeModel = vae(encoderModel, decoderModel);

  for (let i = 0; i < 20; i++) {
    // Shuffle the array of images randomly
    tf.util.shuffle(images);

    // Take a random subset of the specified size
    const randomSubset = images.slice(0, 8);

    const xTrain = tf.concat(randomSubset);
    const xTrainReshaped = xTrain.reshape([xTrain.shape[0], 28 * 28]);

    // Compile the VAE model with the custom loss function
    const optimizer = tf.train.adam();

    optimizer.minimize((): tf.Scalar => {
      const outputs = vaeModel.apply(xTrainReshaped) as tf.Tensor<tf.Rank>[];
      const loss = vaeLoss(xTrainReshaped, outputs);

      return loss;
    })
  }

  // Generate an image after training
  // Generate a preview after each epoch
  await generate(decoderModel, vaeOpts.latentDim);
}

async function generate(decoderModel: tf.LayersModel, latentDimSize: number) {
  const targetZ = tf.zeros([latentDimSize]).expandDims();
  const generated = (decoderModel.predict(targetZ)) as tf.Tensor<tf.Rank>;

  visualizeImage(generated.dataSync() as Float32Array);
}


function visualizeImage(imageData: Float32Array) {
  const canvas = document.createElement('canvas') as HTMLCanvasElement;
  canvas.width = 28;
  canvas.height = 28;
  const ctx = canvas.getContext('2d') as CanvasRenderingContext2D;

  // Normalize the pixel values to [0, 255]
  const pixelData = imageData.map(value => Math.round(value * 255));

  // Create ImageData object
  const imgData = ctx.createImageData(28, 28);

  // Set the image data
  for (let i = 0; i < imgData.data.length; i += 4) {
    const j = i / 4;
    imgData.data[i + 0] = pixelData[j]; // R
    imgData.data[i + 1] = pixelData[j]; // G
    imgData.data[i + 2] = pixelData[j]; // B
    imgData.data[i + 3] = 255; // Alpha
  }

  // Draw ImageData onto the canvas
  ctx.putImageData(imgData, 0, 0);

  // Append canvas to the document body or wherever you want to display it
  document.body.appendChild(canvas);
}

// Initialize
tf.serialization.registerClass(ZLayer);

// const encoderModel = encoder(opts);
// const decoderModel = decoder(opts);
// const vaeModel = vae(encoderModel, decoderModel);

train(opts);